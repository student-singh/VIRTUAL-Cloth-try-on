# -*- coding: utf-8 -*-
"""2D

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12l8GZP4sHmBvCbUvh-P42sTFNbgKui86

# Updated Phase 2 model
"""

# Commented out IPython magic to ensure Python compatibility.
from getpass import getpass
import os

# Set GitHub credentials
os.environ['GITHUB_AUTH'] = "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"

# Clone the repo
!git clone https://$GITHUB_AUTH@github.com/faijan-khan/Phase-2.git
# %cd Phase-2

!pip install ninja

import gdown
import numpy as np
from PIL import Image
import IPython
import gdown
import os
import sys

from predict_pose import generate_pose_keypoints

# Commented out IPython magic to ensure Python compatibility.
'''
gdown.download('https://drive.google.com/uc?id=1tE7hcVFm8Td8kRh5iYRBSDFdvZIkbUIR', 'Data_preprocessing/data.zip', quiet=False)
# %cd Data_preprocessing
!unzip data
# %cd ..
'''

!mkdir Data_preprocessing/test_color
!mkdir Data_preprocessing/test_colormask
!mkdir Data_preprocessing/test_edge
!mkdir Data_preprocessing/test_img
!mkdir Data_preprocessing/test_label
!mkdir Data_preprocessing/test_mask
!mkdir Data_preprocessing/test_pose
!mkdir inputs
!mkdir inputs/img
!mkdir inputs/cloth

# Commented out IPython magic to ensure Python compatibility.
# %cd pose
!gdown --id 1hOHMFHEjhoJuLEQY0Ndurn5hfiA9mwko
# %cd ..

!git clone https://github.com/levindabhi/Self-Correction-Human-Parsing-for-ACGPN.git
!git clone https://github.com/levindabhi/U-2-Net.git

#for segmentation mask generation
url = 'https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH'
output = 'lip_final.pth'
gdown.download(url, output, quiet=False)

# Commented out IPython magic to ensure Python compatibility.
# %cd U-2-Net
!mkdir saved_models
!mkdir saved_models/u2net
!mkdir saved_models/u2netp
!gdown --id 1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy -O saved_models/u2netp/u2netp.pth
!gdown --id 1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ -O saved_models/u2net/u2net.pth
import u2net_load
import u2net_run
u2net = u2net_load.model(model_name = 'u2netp')
# %cd ..

# Commented out IPython magic to ensure Python compatibility.
!mkdir checkpoints
gdown.download('https://drive.google.com/uc?id=1UWT6esQIU_d4tUm8cjxDKMhB8joQbrFx',output='checkpoints/ACGPN_checkpoints.zip', quiet=False)
# %cd checkpoints
!unzip ACGPN_checkpoints
# %cd ..

!wget -O pose/pose_iter_440000.caffemodel \
  https://github.com/nik1806/Human-Pose-Detection/raw/refs/heads/master/model/coco/pose_iter_440000.caffemodel

"""# Gradio interface

"""

# Install gradio if needed
!pip install gradio

# Add this after all the setup cells (after model downloads and directory creation)
import gradio as gr
import os
import shutil
from PIL import Image
import time
import u2net_run  # Ensure U-2-Net is properly imported

def process_images(human_img, cloth_img):
    try:
        start_time = time.time()

        # Debug: Print current directory
        print("Current working directory:", os.getcwd())

        # Create necessary directories
        os.makedirs('inputs/img', exist_ok=True)
        os.makedirs('inputs/cloth', exist_ok=True)
        os.makedirs('Data_preprocessing/test_img', exist_ok=True)
        os.makedirs('Data_preprocessing/test_color', exist_ok=True)
        os.makedirs('Data_preprocessing/test_edge', exist_ok=True)

        # Fixed file paths
        human_path = 'inputs/img/000001_0.png'
        cloth_path = 'inputs/cloth/000001_1.png'
        test_img_path = 'Data_preprocessing/test_img/000001_0.png'
        test_color_path = 'Data_preprocessing/test_color/000001_1.png'
        test_edge_path = 'Data_preprocessing/test_edge/000001_1.png'

        # Convert and save images
        if human_img is not None:
            img = Image.open(human_img)
            img = img.resize((192,256), Image.BICUBIC)
            img.save(human_path)
            img.save(test_img_path)
            print("Human image saved successfully")

        if cloth_img is not None:
            img = Image.open(cloth_img)
            img = img.resize((192,256), Image.BICUBIC).convert('RGB')
            img.save(cloth_path)
            img.save(test_color_path)
            print("Cloth image saved successfully")

        # Generate edge map using U-2-Net
        print("Generating edge map...")
        u2net_run.infer(u2net, 'Data_preprocessing/test_color', 'Data_preprocessing/test_edge')

        # Remove previous results
        shutil.rmtree('results', ignore_errors=True)
        print("Cleaned previous results")

        # Run segmentation
        print("Running segmentation...")
        !python3 Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py \
            --dataset 'lip' \
            --model-restore 'lip_final.pth' \
            --input-dir 'Data_preprocessing/test_img' \
            --output-dir 'Data_preprocessing/test_label'

        # Generate pose keypoints
        print("Generating pose keypoints...")
        pose_path = 'Data_preprocessing/test_pose/000001_0_keypoints.json'
        os.makedirs('Data_preprocessing/test_pose', exist_ok=True)
        generate_pose_keypoints(test_img_path, pose_path)

        # Create test pairs file
        with open('Data_preprocessing/test_pairs.txt','w') as f:
            f.write('000001_0.png 000001_1.png')
        print("Created test pairs file")

        # Run inference
        print("Running inference...")
        !python test.py

        # Load and return result
        result_path = 'results/test/try-on/000001_0.png'
        if os.path.exists(result_path):
            print("Processing completed successfully!")
            print(f"Total time: {time.time() - start_time:.2f} seconds")
            return result_path
        else:
            return "Error: No output generated. Check console for details."

    except Exception as e:
        import traceback
        error_msg = f"Error during processing:\n{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg

# Create Gradio interface
interface = gr.Interface(
    fn=process_images,
    inputs=[
        gr.Image(type="filepath", label="Upload Human Image"),
        gr.Image(type="filepath", label="Upload Cloth Image")
    ],
    outputs=gr.Image(label="Result"),
    title="Virtual Try-On Demo",
    description="Upload a human image and a clothing image to see the virtual try-on result",
    allow_flagging="never"
)

# Launch the interface
interface.launch(debug=True)